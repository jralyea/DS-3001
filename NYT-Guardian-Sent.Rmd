---
title: "NYT-Guardian-Sent"
author: "Jay Ralyea"
date: "3/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Search term: Text mining.

January 1, 2010
The Guardian (London): 14 articles
The New York Times: 6 articles

```{r}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("ggwordcloud")
library(ggwordcloud)
#install.packages('textdata')
library(textdata)
#install.packages('textreadr')
library(textreadr)
```

```{r TestChunk, eval=FALSE}
#################################################################
# Testing chunk. This code is completely irrelevant to the task 
# at hand, other than me testing random methods to complete the 
# assignment
#################################################################

test.doc <- read_rtf("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles/Price Forecasts For Electronics.rtf") %>%
  tibble() %>%
  rename(text = ".")

which(test.doc$. == "Body")
which(test.doc$. == "Classification")

test.slice <- slice(test.doc, 9:20)
unnest_tokens(test.slice, word, text)

test.doc[9:20,]


is.character(test.slice$.)

test.bank <- nyt_read('Price Forecasts For Electronics.rtf')
count(test.bank, word, sort=T)
test.affin <- test.bank %>%
  inner_join(get_sentiments("afinn"))
test.nrc <- test.bank %>%
  inner_join(get_sentiments("nrc"))
test.bing <- test.bank %>%
  inner_join(get_sentiments("bing"))


join_sent <- function(words){
  afinn <- inner_join(words, get_sentiments("afinn"))
  nrc <- inner_join(words, get_sentiments("nrc"))
  bing <- inner_join(words, get_sentiments("bing"))
  cbind(words, afinn, nrc, bing)
}
test.bank <- join_sent(test.bank)

test.guard.list <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles")
# Define blank tibble for relevant words in Guardian articles
test_list <- tibble()

# Add relevant words in Guardian articles to Guardian tibble
for (i in test.guard.list){
  test_list <- test_list %>%
    bind_rows(guardian_read(i))
}
test.guard.list
```


```{r ReadingDataFunctions}
# Path will need to be updated depending on your system
guard_path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles"
nyt_path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles"

# Function to read in the data from the guardian
guardian_read <- function(title){
  path <- paste(guard_path, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") # rename column as "text" instead of "."
  to_text(tib) # calls on function created below
}

# Function to read data from the New York Times(NYT)
nyt_read <- function(title){
  path <- paste(nyt_path, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") # rename column as "text" instead of "."
  to_text(tib) # calls on function created below
}

# Function finds relevant text and un-nests all words into individual rows
# The relevant text begins after the tibble row that contains "Body" and ends
# after the tibble row that contains "Classification." This appears to be true
# for all files downloaded from LexisNexus.
to_text <- function(tib){
  begin.text <- which(tib$text == "Body") + 1 # beginning of relevant text
  end.text <- which(tib$text == "Classification") - 1 # end of relevant text
  tib <- tib[begin.text:end.text, ] # subset to only article text
  unnest_tokens(tib, word, text) %>%
    anti_join(stop_words) # remove stop words from text
}
```

```{r ReadData}
# All guardian article names
guardian_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles")

# All NYT article names
nyt_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles")

# Define blank tibble for relevant words in Guardian articles
guardian_tib <- tibble()

# Add relevant words in Guardian articles to Guardian tibble
for (article in guardian_articles){
  guardian_tib <- guardian_tib %>% # splits articles into single words
    bind_rows(guardian_read(article)) # binds words to end of tibble
}

# Define blank tibble for relevant words in NYT articles
nyt_tib <- tibble()

# Add relevant words in NYT articles to NYT tibble
for (article in nyt_articles){
  nyt_tib <- nyt_tib %>% # splits articles into single words
    bind_rows(nyt_read(article)) # binds words to end of tibble
}

```

```{r SentimentAnalyses}
# The three sentiment analyses for Guardian articles
guard.afinn <- guardian_tib %>%
  inner_join(get_sentiments("afinn"))
guard.nrc <- guardian_tib %>%
  inner_join(get_sentiments("nrc"))
guard.bing <- guardian_tib %>%
  inner_join(get_sentiments("bing"))

# The three sentiment analyses for NYT articles
nyt.afinn <- nyt_tib %>%
  inner_join(get_sentiments("afinn"))
nyt.nrc <- nyt_tib %>%
  inner_join(get_sentiments("nrc"))
nyt.bing <- nyt_tib %>%
  inner_join(get_sentiments("bing"))
```

```{r TableComparison}
# AFINN comparisons
table(guard.afinn$value)
table(nyt.afinn$value)

# Bing comparisons
table(guard.bing$sentiment)
table(nyt.bing$sentiment)

# NRC comparisons
table(guard.nrc$sentiment)
table(nyt.nrc$sentiment)
```

```{r Wordclouds}
# Determine word counts for relevant words in all Guardian articles
guardian.count <- guardian_tib %>%
  count(word, sort = TRUE)
# Create wordcloud for the 50 words with the most mentions
ggplot(guardian.count[1:50, ], aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal()

# Determine word counts for relevant words in all NYT articles
nyt.count <- nyt_tib %>%
  count(word, sort = TRUE)
# Create wordcloud for the 50 words with the most mentions
ggplot(nyt.count[1:50, ], aes(label = word, size = n)) + 
  geom_text_wordcloud() +
  theme_minimal()

```

```{r AFINN-Comparisons}
ggplot(guard.afinn, aes(x = value)) +
  geom_histogram() +
  labs(title = "Guardian Sentiment Range") +
  theme_bw()

ggplot(nyt.afinn, aes(x = value)) +
  geom_histogram() +
  labs(title = "New York Time Sentiment Range") +
  theme_bw()
```

```{r TF-IDF}
raw_read <- function(folder, title){
  folder_path <- folder
  path <- paste(folder_path, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") %>% # rename column as "text" instead of "." 
    filter(text != "ABSTRACT" & text != "FULL TEXT")
  relevant_words(tib)
}

relevant_words <- function(tib){
  begin.text <- which(tib$text == "Body") + 1 # beginning of relevant text
  end.text <- which(tib$text == "Classification") - 1 # end of relevant text
  tib[begin.text:end.text, ] # subset to only article text
}

guard_raw <- tibble()
for (title in guardian_articles){
  guard_raw <- guard_raw %>%
    bind_rows(raw_read(guard_path, title))
}

nyt_raw <- tibble()
for (title in nyt_articles){
  nyt_raw <- nyt_raw %>%
    bind_rows(raw_read(nyt_path, title))
}

# Data prep to allow TF-IDF operation on Guardian and NYT
data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}




```































