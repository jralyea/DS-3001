---
title: "Text Mining Sentiment"
author: "William cull, John Hope, and Jay Ralyea"
date: "3/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(textdata)
library(textreadr)
```

# Introduction


```{r JayRalyea}
# Path will need to be updated depending on your system
guard.path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles"
nyt.path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles"

# Function to read in rtf files based on folder location and article title
rtf_read <- function(folder, title){
  path <- paste(folder, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") # rename column as "text" instead of "."
  to_text(tib) # calls on function created below
}

# Function finds relevant text and un-nests all words into individual rows
# The relevant text begins after the tibble row that contains "Body" and ends
# after the tibble row that contains "Classification." This appears to be true
# for all files downloaded from LexisNexus.
to_text <- function(tib){
  begin.text <- which(tib$text == "Body") + 1 # beginning of relevant text
  end.text <- which(tib$text == "Classification") - 1 # end of relevant text
  tib <- tib[begin.text:end.text, ] # subset to only article text
  unnest_tokens(tib, word, text) %>%
    anti_join(stop_words) # remove stop words from text
}

# All guardian article names
guardian_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles")

# All NYT article names
nyt_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles")

# Define blank tibble for relevant words in Guardian articles
Guard_files <- tibble()

# Add relevant words in Guardian articles to Guardian tibble
for (article in guardian_articles){
  Guard_files <- Guard_files %>% # splits articles into single words
    bind_rows(rtf_read(guard.path, article)) # binds words to end of tibble
}

# Count for each word in Guardian articles
Guard_files <- Guard_files %>%
  count(word, sort = TRUE)

# Define blank tibble for relevant words in NYT articles
NYT_files <- tibble()

# Add relevant words in NYT articles to NYT tibble
for (article in nyt_articles){
  NYT_files <- NYT_files %>% # splits articles into single words
    bind_rows(rtf_read(nyt.path, article)) # binds words to end of tibble
}

# Count for each word in NYT articles
NYT_files <- NYT_files %>%
  count(word, sort = TRUE)

```


```{r JohnHope}
# Folder CE and CHE files are saved in
folder <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/All Articles"

# Title of each newspaper
titles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/All Articles")

# The Spanish One
CE_files <- read_rtf(paste(folder, titles[1], sep = "/"))
CE_files <- tibble(CE_files)

# Removing lines w/ information between articles information

CE_files <- CE_files[-(c(1:66, 110:118, 130:138, 147:155, 197:205, 237:245, 260:268, 277:285, 296:297)),]

CE_files <- CE_files %>%
  unnest_tokens(word, CE_files) %>%
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

# Chronicle of Higher Education
CHE_files <- read_rtf(paste(folder, titles[2], sep = "/"))
CHE_files <- tibble(CHE_files)
CHE_files <- CHE_files[-(c(1:71, 97:103, 142:148, 177:183, 229:235, 299:305, 364:370, 410:416, 449:455, 479:480)),]

CHE_files <- CHE_files %>%
  unnest_tokens(word, CHE_files) %>%
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)
```

```{r BillCull}
UW <- read_lines(paste(folder, titles[3], sep = "/"))
UW <- tibble(UW)
UW$UW <- as.character(UW$UW)
UW_files <- UW %>%
  rename(text = "UW") %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) 
```

```{r Sentiments}
# Sentiment Function
sent_func <- function(tib, senti){
  tib %>%
    inner_join(get_sentiments(senti))
}

# The Guardian Sentiments
guard.afinn <- sent_func(Guard_files, "afinn")
guard.nrc <- sent_func(Guard_files, "nrc")
guard.bing <- sent_func(Guard_files, "bing")

# The New York Times sentiments
nyt.afinn <- sent_func(NYT_files, "afinn")
nyt.nrc <- sent_func(NYT_files, "nrc")
nyt.bing <- sent_func(NYT_files, "bing")

# CE Noticias Financieras English sentiments
ce.afinn <- sent_func(CE_files, "afinn")
ce.nrc <- sent_func(CE_files, "nrc")
ce.bing <- sent_func(CE_files, "bing")

# The Chronicle of Higher Education sentiments
che.afinn <- sent_func(CHE_files, "afinn")
che.nrc <- sent_func(CHE_files, "nrc")
che.bing <- sent_func(CHE_files, "bing")

# University Wire sentiments
uw.afinn <- sent_func(UW_files, "afinn")
uw.nrc <- sent_func(UW_files, "nrc")
uw.bing <- sent_func(UW_files, "bing")
```

# Wordclouds {.tabset}
## The Guardian
```{r}
# Create wordcloud for the 50 words with the most mentions
wordcloud <- function(file){
  ggplot(file[1:50, ], aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal()
}

# Guardian Wordcloud
wordcloud(Guard_files)
```
## The New York Times
```{r}
# NYT Wordcloud
wordcloud(NYT_files)
```
## CE Noticias Financieras English
```{r}
# CE Wordcloud
wordcloud(CE_files)
```
## The Chronicle of Higher Education
```{r}
# CHE Wordcloud
wordcloud(CHE_files)
```

## University Wire
```{r}
# UW Wordcloud
wordcloud(UW_files)
```


# AFINN Histograms {.tabset}
## The Guardian
```{r}
# Define a function to create the histograms for each newspaper
histo <- function(afinn){
  ggplot(afinn, aes(x = value)) +
    geom_histogram() + 
    labs(x = "Sentiment Value",
         y = "Value Count") +
    theme_bw()
}

# Guardian Histogram
histo(guard.afinn)
```

## The New York Times
```{r}
# NYT Histogram
histo(nyt.afinn)
```

## CE Noticias Financieras English
```{r}
# CE Histogram
histo(ce.afinn)
```

## The Chronicle of Higher Education
```{r}
# CHE Histogram
histo(che.afinn)
```

## University Wire
```{r}
# UW Histogram
histo(uw.afinn)
```




































