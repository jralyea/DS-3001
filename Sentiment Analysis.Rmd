---
title: "Text Mining Sentiment"
author: "William cull, John Hope, and Jay Ralyea"
date: "3/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(textdata)
library(textreadr)
```

# Introduction


```{r JayRalyea-Clean}
### CLEANING ARTICLES FROM THE GUARDIAN AND THE NEW YORK TIMES ###

# Path will need to be updated depending on your system
guard.path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles"
nyt.path <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles"

# Function to read in rtf files based on folder location and article title
rtf_read <- function(folder, title){
  path <- paste(folder, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") # rename column as "text" instead of "."
  to_text(tib) # calls on function created below
}

# Function finds relevant text and un-nests all words into individual rows
# The relevant text begins after the tibble row that contains "Body" and ends
# after the tibble row that contains "Classification." This appears to be true
# for all files downloaded from LexisNexus.
to_text <- function(tib){
  begin.text <- which(tib$text == "Body") + 1 # beginning of relevant text
  end.text <- which(tib$text == "Captions:" | tib$text == "Classification") - 1 # end of relevant text
  tib <- tib[begin.text:end.text, ] # subset to only article text
  unnest_tokens(tib, word, text) %>%
    anti_join(stop_words) # remove stop words from text
}

# All guardian article names
guardian_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/Guardian-Articles")

# All NYT article names
nyt_articles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/NYT-Articles")

# Define blank tibble for relevant words in Guardian articles
Guard_files <- tibble()

# Add relevant words in Guardian articles to Guardian tibble
for (article in guardian_articles){
  Guard_files <- Guard_files %>% # splits articles into single words
    bind_rows(rtf_read(guard.path, article)) # binds words to end of tibble
}

# Count for each word in Guardian articles
Guard_files <- Guard_files %>%
  count(word, sort = TRUE)

# Define blank tibble for relevant words in NYT articles
NYT_files <- tibble()

# Add relevant words in NYT articles to NYT tibble
for (article in nyt_articles){
  NYT_files <- NYT_files %>% # splits articles into single words
    bind_rows(rtf_read(nyt.path, article)) # binds words to end of tibble
}

# Count for each word in NYT articles
NYT_files <- NYT_files %>%
  count(word, sort = TRUE)

```


```{r JohnHope-Clean}
### CLEANING ARTICLES FROM CE NOTICIAS FINANCIERAS ENGLISH ###
###          AND THE CHRONICLE OF HIGHER EDUCATION         ###

# Folder CE and CHE files are saved in
folder <- "C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/All Articles"

# Title of each newspaper
titles <- list.files("C:/Users/Student/Documents/Third Year Sem 2/DS 3001/DS-3001/All Articles")

# CE Noticias Financieras
CE_files <- read_rtf(paste(folder, titles[1], sep = "/"))
CE_files <- tibble(CE_files)

# Removing lines w/ information between articles information
CE_files <- CE_files[-(c(1:66, 110:118, 130:138, 147:155, 197:205, 237:245, 260:268, 277:285, 296:297)),]

# Save raw file lines for TF-IDF
CE_raw <- CE_files

CE_files <- CE_files %>%
  unnest_tokens(word, CE_files) %>%
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)

# Chronicle of Higher Education
CHE_files <- read_rtf(paste(folder, titles[2], sep = "/"))
CHE_files <- tibble(CHE_files)

# Removing lines w/ useless information between articles
CHE_files <- CHE_files[-(c(1:71, 97:103, 142:148, 177:183, 229:235, 299:305, 364:370, 410:416, 449:455, 479:480)),]

# Save raw file lines for TF-IDF
CHE_raw <- CHE_files

CHE_files <- CHE_files %>%
  unnest_tokens(word, CHE_files) %>%
  anti_join(stop_words) %>% 
  count(word, sort=TRUE)
```

```{r BillCull-Clean}
### CLEANING ARTICLES FROM UNIVERSITY WIRE ###

# Read in lines from txt file. Title is third element of titles vector
UW <- read_lines(paste(folder, titles[3], sep = "/"))

# Place lines in tibble and ensure values are characters
UW <- tibble(UW)
UW$UW <- as.character(UW$UW)

# Rename column, unnest words, remove stop words, 
# and determine count for each word
UW_files <- UW %>%
  rename(text = "UW") %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)  
```

```{r Sentiments}
# Sentiment Function
sent_func <- function(tib, senti){
  tib %>%
    inner_join(get_sentiments(senti))
}

# The Guardian Sentiments
guard.afinn <- sent_func(Guard_files, "afinn")
guard.nrc <- sent_func(Guard_files, "nrc")
guard.bing <- sent_func(Guard_files, "bing")

# The New York Times sentiments
nyt.afinn <- sent_func(NYT_files, "afinn")
nyt.nrc <- sent_func(NYT_files, "nrc")
nyt.bing <- sent_func(NYT_files, "bing")

# CE Noticias Financieras English sentiments
ce.afinn <- sent_func(CE_files, "afinn")
ce.nrc <- sent_func(CE_files, "nrc")
ce.bing <- sent_func(CE_files, "bing")

# The Chronicle of Higher Education sentiments
che.afinn <- sent_func(CHE_files, "afinn")
che.nrc <- sent_func(CHE_files, "nrc")
che.bing <- sent_func(CHE_files, "bing")

# University Wire sentiments
uw.afinn <- sent_func(UW_files, "afinn")
uw.nrc <- sent_func(UW_files, "nrc")
uw.bing <- sent_func(UW_files, "bing")
```

# Wordclouds {.tabset}
## The Guardian
```{r}
# Create wordcloud for the 50 words with the most mentions
wordcloud <- function(file){
  ggplot(file[1:50, ], aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal()
}

# Guardian Wordcloud
wordcloud(Guard_files)
```
## The New York Times
```{r}
# NYT Wordcloud
wordcloud(NYT_files)
```
## CE Noticias Financieras English
```{r}
# CE Wordcloud
wordcloud(CE_files)
```
## The Chronicle of Higher Education
```{r}
# CHE Wordcloud
wordcloud(CHE_files)
```

## University Wire
```{r}
# UW Wordcloud
wordcloud(UW_files)
```


# AFINN Histograms {.tabset}
## The Guardian
```{r}
# Define a function to create the histograms for each newspaper
histo <- function(afinn){
  ggplot(afinn, aes(x = value)) +
    geom_histogram() + 
    labs(x = "Sentiment Value",
         y = "Value Count") +
    theme_bw()
}

# Guardian Histogram
histo(guard.afinn)
```

## The New York Times
```{r}
# NYT Histogram
histo(nyt.afinn)
```

## CE Noticias Financieras English
```{r}
# CE Histogram
histo(ce.afinn)
```

## The Chronicle of Higher Education
```{r}
# CHE Histogram
histo(che.afinn)
```

## University Wire
```{r}
# UW Histogram
histo(uw.afinn)
```



```{r Raw-TF-IDF-Jay}
# Raw files need to be found for The Guardian and The New York Times
# as they were not defined previously

# Create tibble of article lines
raw_read <- function(folder, title){
  folder_path <- folder
  path <- paste(folder_path, title, sep = "/") #combine title of article w/ path
  tib <- path %>%
    read_rtf() %>%
    tibble() %>%
    rename(text = ".") %>% # rename column as "text" instead of "." 
    filter(text != "ABSTRACT" & text != "FULL TEXT") # some articles have an abstract and full text sub-headers, remove them
  relevant_words(tib)
}

# Find the relevant sections in the article
relevant_words <- function(tib){
  begin.text <- which(tib$text == "Body") + 1 # beginning of relevant text
  end.text <- which(tib$text == "Captions:" | tib$text == "Classification") - 1 # end of relevant text
  tib[begin.text:end.text, ] # subset to only article text
}

# Generate tibble of raw data for The Guardian
Guard_raw <- tibble()
for (title in guardian_articles){
  Guard_raw <- Guard_raw %>%
    bind_rows(raw_read(guard.path, title))
}

# Generate tibble of raw data for The New York Times
NYT_raw <- tibble()
for (title in nyt_articles){
  NYT_raw <- NYT_raw %>%
    bind_rows(raw_read(nyt.path, title))
}

```

```{r Raw-TF-IDF-Bill}
# Find raw files for University Wire as it was not done previously
UW_raw <- UW %>%
  rename(text = "UW") %>%
  filter(text != "")
```


```{r TF-IDF-Eval}
# Data prep to allow TF-IDF operation on newspapers
data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

# Determine necessary end value for the data_prep function
# This returns a value of "V(NUM ROWS IN TIBBLE)"
end_val <- function(tib){
  end.row <- as.character(nrow(tib))
  paste("V", end.row, sep = "")
}

# Create bags for the newspapers
Guard_bag <- data_prep(Guard_raw, 'V1', end_val(Guard_raw))
NYT_bag <- data_prep(NYT_raw, 'V1', end_val(NYT_raw))
CE_bag <- data_prep(CE_raw, 'V1', end_val(CE_raw))
CHE_bag <- data_prep(CHE_raw, 'V1', end_val(CHE_raw))
UW_bag <- data_prep(UW_raw, 'V1', end_val(UW_raw))

# Newspapers
newspaper <- c("The Guardian", "The New York Times", 
               "CE Noticias Financieras", "The Chronicle of Higher Ed",
               "University Wire")

# Necessary text to run tf-idf evaluation
tf_idf_text <- tibble(newspaper, text = t(tibble(Guard_bag, NYT_bag,
                                                 CE_bag, CHE_bag, UW_bag,
                                                 .name_repair = "universal")))

# Un-nest words from each newspaper, remove stop words, and add word counts
word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(newspaper, word, sort = TRUE)

# Group total word count by newspaper
total_words <- word_count %>% 
  group_by(newspaper) %>% 
  summarize(total = sum(n))

# Join together word count and total words
news_words <- left_join(word_count, total_words)

# Add tf-idf evaluation to the list of words
news_words <- news_words %>%
  bind_tf_idf(word, newspaper, n)

```





























